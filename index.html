
<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>ABCDE ML</title>
		<meta charset="utf-8" />
		
	</head>
<body>

	<h1>36-708: The ABCDE of Statistical Methods in Machine Learning</h1>
	<h2>(aka: developing judgment for complex stat-ml methods)</h2>

<h2>Class location: DH 1211 (MW 10:30-11:50am)</h2>
	
<h3> Office hours locations: Aaditya: BH 132H (MW 12-12:30pm), Pratik: GHC 8106 (T 2-3pm)</h3>

<b>Syllabus:</b>	
<a href="Syllabus.pdf">Tentative course syllabus</a><br><br>

<b>Lectures:</b><br>

<ul>
<li> L01 (Jan 13): Introduction <br>
<li> L02 (Jan 15): Basics: regression, classification <br>
<li> L-- (Jan 20): No class (MLK day) <br>
<li> L03 (Jan 22): Nearest-neighbor methods: k-nn regression and classification [B] <br>
<li> L04 (Jan 27): Uncertainty quantification: conformal prediction [E] <br>
<li> L05 (Jan 29): Ensemble methods: boosting (game-theoretic perspective) [A] <br>
<li> L06 (Feb 03): Ensemble methods: boosting (statistical perspective) [B] <br>
<li> L07 (Feb 05): Ensemble methods: boosting (computational considerations, applications) [C, D], guest lecture by Allie <br>
<li> L08 (Feb 10): Ensemble methods: boosting (generationalization) [B] <br>
<li> L09 (Feb 12): Quiz 1  <br>
<li> L10 (Feb 17): Ensemble methods: bagging, random forest [A, B] <br>
<li> L11 (Feb 19): Variable importance: random forest (E) <br>
<li> L12 (Feb 24): Datapoint importance: shapely values (E) <br>
<li> L13 (Feb 26): Ensemble methods: stacking (A) <br>
<li> L14 (Mar 02):  <br>
<li> L15 (Mar 04):  <br>
<li> L16 (Mar 09):  <br>
<li> L17 (Mar 11):  <br>
<li> L18 (Mar 16):  <br>
<li> L19 (Mar 18):  Quiz 2 <br>
<li> L20 (Mar 23):  <br>
<li> L21 (Mar 25):  <br>
<li> L22 (Mar 30):  <br>
<li> L23 (Apr 01):  <br>
<li> L24 (Apr 06):  <br>
<li> L25 (Apr 13):  <br>
<li> L26 (Apr 15):  Quiz 3 <br>
<li> L27 (Apr 20):  <br>
<li> L28 (Apr 22):  <br>
<li> L29 (Apr 27):  <br>
<li> L30 (Apr 29):  <br><br>
</ul>

<b>Notes:</b><br>

<ul>
<li> <a href="https://docs.google.com/spreadsheets/d/1m4uRV-lpLqNeFVz6dLczAQ4O6WMuQc1tg1AHvkMHc8o/edit#gid=0">Crowd-scribing sign-up sheet (access with a CMU account)</a><br>

<li> <a href="https://www.overleaf.com/read/jzhmrrfgfnkh">Crowd-scribed class notes (read-only; ask for edit link if in class)</a><br><br>	
</ul>

<b>Homeworks:</b><br>
	
<ul>
<li> <a href="https://www.dropbox.com/s/ggxiqnojsoi4er3/homework_1.pdf?dl=0">Homework 1</a><br>
<li> <a href="https://www.dropbox.com/s/0hba8tiaguuqoqa/homework_2.pdf?dl=0"><font color=red>Homework 2</font></a><br><br>
</ul>

<b>Quizzes:</b><br>
	
<ul>
<li> <a href="https://www.dropbox.com/s/bduyg8ffh6de0yr/Exam1.pdf?dl=0">Quiz 1</a> (<a href="1nn_voronoi.html">question 4 clarification</a>; <a href="grades_exam1.html">class performance</a>)<br><br>
</ul>

<b>Course philosophy (ABCDE):</b> This course focuses on statistical methods for machine learning, a decades-old topic in statistics that now has a life of its own, intersecting with many other fields. While the core focus of this course is methodology (algorithms), the course will have some amount of formalization and rigor (theory/derivation/proof), and some amount of interacting with data (simulated and real). However, the primary way in which this course complements related courses in other departments is the joint ABCDE focus on 
<ul>
<li>(A) Algorithm design principles,<br> 
<li>(B) Bias-variance thinking, <br>
<li>(C) Computational considerations <br>
<li>(D) Data analysis <br>
<li>(E) Explainability and interpretability. <br><br>
</ul>


<b>Non-technical blurb:</b> In the instructor’s opinion, (B) is the most important — every day, researchers come up with yet another new algorithm/model, scale it up by using distributed computing and stochastic optimization, and throw it at a big real dataset (A, C, D). However, in the era of big data, big bias and big variance is a big issue! Instead of producing just predictions, uncertainty quantification is critical for applications (how sure are we of these predictions?). Blindly throwing lots of data and complex black-box models at a problem might produce initially promising results, but the results may be highly variable and non-robust to minor changes in the data or tuning parameters. Importantly, more data does not eliminate bias — "obvious" bias caused by covariate shift or outliers, and "subtle" bias like selection bias, sample bias, confirmation bias, etc. Understanding the variety of different sources of bias and variance, and the effects they can have on the final outputs, is a critical component of using ML algorithms in practice, and will be a central theme of the course. Of course, (E) is also important and often underemphasized, and we will cover some recent methods for interpreting models such as measures for variable importance and/or data-point importance. <br><br>

<b>Technical blurb:</b> The course will cover (some) classical and (some) modern methods in statistical machine learning; the field is so vast that the qualifier "some" is critical. These include unsupervised learning (dimensionality reduction, clustering, generative modeling, etc) and supervised learning (classification, regression, etc). Time permitting we might cover dynamic forms of learning (active learning, reinforcement learning, etc). We will assume basic familiarity with linear/parametric methods, and dwell more on nonlinear/nonparametric methods (kernels, random forests, boosting, neural nets, etc). <br><br>

<b>Critical thinking:</b> Unlike other courses, we will not just list one algorithm after another. Instead, we will work on developing some skepticism when using these methods by asking more nuanced questions. When do these methods “work”, why do they work, and why might they fail? Can we quantitatively measure if they are “working” or “failing”? Rather than just making a prediction, how can we quantify uncertainty of our predictions? How do we compare different regression methods or classification algorithms? How do we select a model from a nested class of models of increasing complexity? Are prediction algorithms useful for hypothesis testing? How can we interpret complex models, for example: what are measures of variable importance and data-point importance? These questions do not all have easy or straightforward answers, but various attempts at formalization and analysis will nevertheless be discussed (and will naturally lead to course projects, and potentially research projects).<br><br>


<b>Who should take it:</b> This is a required first year PhD course in the Department of Statistics and Data Science, and so the course will be taught at that level. However, other PhD students from other departments are welcome to attend (as credit or audit) with instructor permission. <br><br>


<b>Prerequisites:</b> At the very least, all students should have taken Intermediate Statistics (36705), be proficient at programming in R and/or Python and/or Matlab, and be comfortable with linear algebra, probability, calculus and related topics (see resources below that you should be familiar with). 36700 could possibly substitute for 36705 with permission. Students who have taken 10701, 10715 or 10716 can still take this course, since there are likely to be many complementary and non-intersecting topics. Apart from the unique angle taken by this course, the smaller size of class will ensure more individual attention and instructor interaction, so attendance (especially for crediting) will be selective. 
<br><br>


<b>Instructor:</b> <a href="http://stat.cmu.edu/~aramdas/">Aaditya Ramdas</a> is a core faculty in the Department of Statistics and Data Science and in the Machine Learning Department. He did his postdoc at Berkeley (joint in these two departments) and his PhD at CMU (joint in these two departments). He is an expert on statistical machine learning, sequential inference and multiple testing.<br><br>

<b>Some textbooks:</b> <br>
<ul>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning: Data Mining, Inference and Prediction.</a> Trevor Hastie, Robert Tibshirani, Jerome Friedman.<br>
<li><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">An Introduction to Statistical Learning: With Applications in R.</a> Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.<br>
<li><a href="https://cs.nyu.edu/~mohri/mlbook/">Foundations of Machine Learning.</a> Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.<br><br>
</ul>


<b>References to catch up on prerequisites:</b>
<ul>
	<li><a href=https://www.youtube.com/channel/UCu8Pv6IJsbQdGzRlZ5OipUg/videos>Videos of Larry Wasserman's 36-705 course</a>

  <li><a
  href="http://www.cs.cmu.edu/~zkolter/course/linalg/index.html">Linear 
  algebra review</a>, videos by Zico Kolter
  
  <li><a
  href="https://www.youtube.com/channel/UC7gOYDYEgXG1yIH_rc2LgOw/playlists">Real 
  analysis, calculus, and more linear algebra</a>, videos by Aaditya Ramdas 

  <li><a href="prerequisite_topics.pdf">Convex optimization prequisites 
  review</a> from Spring 2015 course, by Nicole Rafidi

  <li>See also Appendix A of <a
  href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Boyd and
  Vandenberghe (2004) for general mathematical review  
</ul>
<br><br>


</body>

</html>
